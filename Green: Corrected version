import nltk

from nltk.corpus import gutenberg, stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter

nltk.download("punkt")
nltk.download("gutenberg")
nltk.download("stopwords")
nltk.download("averaged_perceptron_tagger")
nltk.download("universal_tagset")
nltk.download("wordnet")

# Access book text from Gutenberg project
book_text = gutenberg.raw("austen-emma.txt")

# Tokenize text into sentences
sentence_tokens = nltk.sent_tokenize(book_text)

# Tokenize sentences into lists of words
sentences_and_words = []
filtered_word_tokens_with_stopwords = []
filtered_word_tokens = []
stopwords = set(stopwords.words("english"))
unwanted_punctuation = ["!", "?", ",", "--", ".", "''", ";", ":",
                        "'s", "``", "n't", "(", ")", "_", "]", "["]

for sentence in sentence_tokens:
    unfiltered_word_tokens = nltk.word_tokenize(sentence)
    for word in unfiltered_word_tokens:
        # Filter out punctuation and uninteresting words
        if word not in unwanted_punctuation:
            filtered_word_tokens_with_stopwords.append(word)
        if word not in unwanted_punctuation and word.lower() not in stopwords:
            filtered_word_tokens.append(word)

# Count total wordcount in book including stopwords
word_count = len(filtered_word_tokens_with_stopwords)
print(f"{word_count} words in total")

# Count total number of sentences in book
sentence_count = len(sentence_tokens)
print(f"{sentence_count} sentences in total")

# Count number of unique words in book including stop words
unique_word_count = Counter(filtered_word_tokens_with_stopwords)
print(f"{len(unique_word_count)} unique words in total")

# Retrieve and rank the top 10 most used words
top_ten = unique_word_count.most_common(10)
print("top 10:")
rank = 1
for word, count in top_ten:
    print(f"{rank}. {word}, {count} times")
    rank += 1

# Part-of-speech tag word tokens 
tagged_word_tokens = nltk.pos_tag(filtered_word_tokens, tagset="universal")

# Word Type Mapping
word_type = {
  'NOUN': 'n',
  'VERB': 'v',
  'ADJ': 'a',
  'ADV': 'r'
}

# Lemmatize word tokens
lemmatizer = WordNetLemmatizer()
lemmatized_words = []
for word, tag in tagged_word_tokens:
    if tag in word_type:
        lemmatized_words.append(
            lemmatizer.lemmatize(word, pos=word_type[tag])
            )

# Lemmatized words with their tag
tagged_lemmatized_words = nltk.pos_tag(lemmatized_words, tagset="universal")

# Calculate new (lemmatized) wordcount
lemmatized_wordcount = len(lemmatized_words)
print(f"{lemmatized_wordcount} words in total (lemmatized)")

# Calculate new (lemmatized) unique word count
lemmatized_unique_word_count = Counter(lemmatized_words)
# print(lemmatized_unique_word_count)

# Calculate word frequency of nouns
nouns = []
for word, tag in tagged_lemmatized_words:
    if tag == "NOUN":
        nouns.append(word)
nouns_frequency = Counter(nouns)
# print(f"Nouns: {nouns_frequency}")

# Retrieve 5 most common nouns
top_ten = nouns_frequency.most_common(5)
print("Top 5 most common nouns:")
rank = 1
for word, count in top_ten:
    print(f"{rank}. {word}, cited {count} times")
    rank += 1

# Calculate word frequency of verbs
verbs = []
for word, tag in tagged_lemmatized_words:
    if tag == "VERB":
        verbs.append(word)
verbs_frequency = Counter(verbs)
# print(f"Verbs: {verbs_frequency}")

# Retrieve 5 most common verbs
top_ten = verbs_frequency.most_common(5)
print("Top 5 most common verbs:")
rank = 1
for word, count in top_ten:
    print(f"{rank}. {word}, cited {count} times")
    rank += 1

# Calculate word frequency of adjectives
adjectives = []
for word, tag in tagged_lemmatized_words:
    if tag == "ADJ":
        adjectives.append(word)
adjectives_frequency = Counter(adjectives)
# print(f"Adjectives: {adjectives_frequency}")

# Retrieve 5 most common adjectives
top_ten = adjectives_frequency.most_common(5)
print("Top 5 most common adjectives:")
rank = 1
for word, count in top_ten:
    print(f"{rank}. {word}, cited {count} times")
    rank += 1

# Calculate word frequency of adverbs
adverbs = []
for word, tag in tagged_lemmatized_words:
    if tag == "ADV":
        adverbs.append(word)
adverbs_frequency = Counter(adverbs)
# print(f"Adverbs: {adverbs_frequency}")

# Retrieve 5 most common adverbs
top_ten = adverbs_frequency.most_common(5)
print("Top 5 most common adverbs:")
rank = 1
for word, count in top_ten:
    print(f"{rank}. {word}, cited {count} times")
    rank += 1
