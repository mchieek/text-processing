import nltk

nltk.download("punkt")
nltk.download("gutenberg")
nltk.download("stopwords")
nltk.download('averaged_perceptron_tagger')  # You need to do this once if you want to use the pos_tag function
nltk.download('universal_tagset')  # You need to do this once if you want to use the universal tagset
nltk.download('wordnet')  # You need to do this once if you want to use the WordNetLemmatizer

from nltk.corpus import gutenberg, stopwords
from collections import Counter
from nltk.stem import WordNetLemmatizer

# Access book text from Gutenberg project
emma = gutenberg.raw("austen-emma.txt")

# Tokenize text into sentences and words
sentence_tokens = nltk.sent_tokenize(emma)
word_tokens = nltk.word_tokenize(emma)

# Tokenize sentences into lists of words
sentences_and_words = []
word_tokens_filtered = []
word_no_punctuation = []
stopwords = set(stopwords.words("english"))
for sentence in sentence_tokens:
    word_tokens = nltk.word_tokenize(sentence)
    for word in word_tokens:
        # Filter out punctuation and uninteresting words
        if (word.lower() not in ["!", "?", ",", "--", ".", "''", ";", ":", "'s", "``", "n't"] and word.lower() not in stopwords):
            sentences_and_words.append(word)
            word_tokens_filtered.append(word)

# Part-of-Speech tag word tokens
tagged_words = nltk.pos_tag(word_tokens_filtered, tagset="universal")

# Word Type Mapping
word_type = {
  'NOUN': 'n',
  'VERB': 'v',
  'ADJ': 'a',
  'ADV': 'r'
}

# Lemmatize word tokens
lemmatizer = WordNetLemmatizer()
lemmatized_words = []
for word, tag in tagged_words:
    if tag in word_type:
        lemmatized_words.append(
            lemmatizer.lemmatize(word.lower(), pos=word_type[tag])
        )

print(set(lemmatized_words))

# Count total number of words in book
for word in word_tokens:
    if word.lower() not in ["!", "?", ",", "--", ".", "''", ";", ":", "'s", "``", "n't"]:
        word_no_punctuation.append(word)
word_count = len(word_no_punctuation)
print(f"{word_count} words in total")

# Count number of unique words in book
unique_word_frequency = Counter(word_tokens_filtered)
print(f"{len(unique_word_frequency)} unique words in total")

# Retrieve top 10 most used words
top_ten = unique_word_frequency.most_common(10)
print(f"top 10: {[word[0] for word in top_ten]}")
